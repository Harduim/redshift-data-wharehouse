# Data Warehousing on Redshift | Udacity Project 03

Main goals of this project:  
- To Define fact and dimension tables using the star schema for analytics
- Migrate JSON data from S3 buckets to Redshift using Python and SQL.

## Project Datasets

### Song Dataset 

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). 
Each file is in JSON format and contains metadata about a song and the artist of that song.
#### s3://udacity-dend/song_data

### Log Dataset
The second dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

#### s3://udacity-dend/log_data

## Setting up the environment:

### Requirements:
- An active Redshift cluster, instructions on how to setup this: [Here](https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html)
- [Anaconda](https://www.anaconda.com/)

### Conda
- Navigate to the project folder
- Create the environment: `conda env create -f environment.yml`
- Activate the new environment: `conda activate redshift_dwh`

### Database
- Create a new config file named **dwh.json** using the template config file **dwh.json.template**
- Activate the conda environment, if not already active: `conda activate redshift_dwh`
- Navigate to the project folder
- Create the tables: `python3 create_tables.py`

## ETL
**Warning!** Because of the dataset size the process will take hours, so if you have access to an Linux environment I recomend runnig it under [Screen](https://linuxize.com/post/how-to-use-linux-screen/) or [Tmux](https://github.com/tmux/tmux/wiki).

If you dont have the time or just want to check if everything works, change the **song_data** value on the config file from **s3://udacity-dend/song_dat"** to **s3://udacity-dend/song_data/A/A/A**

- Run the ETL pipeline: `python3 etl.py`

